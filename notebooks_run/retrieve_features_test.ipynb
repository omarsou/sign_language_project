{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"retrieve_features_test.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1S-XuiT6kioVAMRYP6CBl9SpmDZFjq97j","authorship_tag":"ABX9TyP8fLLzd5jXHODkMtOIAZmZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"94b118cfd1c3457899a4b48f9f729fa9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8d5f35c09c1f4b3b9544e022cc4904b9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c898455c09f849d7948121528c07cdc5","IPY_MODEL_18b42311cc95479880501eb6380a01e7"]}},"8d5f35c09c1f4b3b9544e022cc4904b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c898455c09f849d7948121528c07cdc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_eef564e7cfac4591a7c0a46cd34930f4","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":642,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":642,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ed03b27b9df44255be3e795a0d67d42d"}},"18b42311cc95479880501eb6380a01e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e5dc51782d794644a0073d835fe02509","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 642/642 [1:07:09&lt;00:00,  6.28s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cd45c382d919409cabe133c8d0d0720c"}},"eef564e7cfac4591a7c0a46cd34930f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ed03b27b9df44255be3e795a0d67d42d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e5dc51782d794644a0073d835fe02509":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cd45c382d919409cabe133c8d0d0720c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"680fbaead58649308ce5a420b93e8dbf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5e510764c7484e149f8019bc6cec1931","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c6db1b979fb641d2bb66078cb1528dc2","IPY_MODEL_12485cc6b4d54323a0ef0133fe25aa1c"]}},"5e510764c7484e149f8019bc6cec1931":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c6db1b979fb641d2bb66078cb1528dc2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0d2dd583a0954ddf84e5ad83bc77f5bc","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":642,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":642,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c6466ec6fc04c16bcf9ea2fc95f194b"}},"12485cc6b4d54323a0ef0133fe25aa1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2dd34d54af354230bc5f03125f65b36a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 642/642 [00:46&lt;00:00, 13.78it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5233e2d1d58749bfb9dc9314b0c9464c"}},"0d2dd583a0954ddf84e5ad83bc77f5bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0c6466ec6fc04c16bcf9ea2fc95f194b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2dd34d54af354230bc5f03125f65b36a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5233e2d1d58749bfb9dc9314b0c9464c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rRXsD0soysZN","executionInfo":{"status":"ok","timestamp":1609068055665,"user_tz":-60,"elapsed":970,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"8be36436-1d72-47be-b466-7d83f445a89b"},"source":["!nvidia-smi -L"],"execution_count":1,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-6c6ca961-4bb4-5b92-95d2-e79dcf3f36b4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WcGl0QXu0wmZ","executionInfo":{"status":"ok","timestamp":1609068059195,"user_tz":-60,"elapsed":1274,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["import sys\n","sys.path.append('/content/drive/MyDrive/sign_language/lcrnet-v2-improved-ppi')\n","#from lcr_net_ppi_improved import LCRNet_PPI_improved\n","sys.path.append('/content/drive/MyDrive/sign_language')\n","import gzip\n","import pickle\n","import sys, os\n","import argparse\n","import os.path as osp\n","from PIL import Image\n","import cv2\n","import numpy as np\n","import time\n","\n","import torch\n","from torchvision.transforms import ToTensor\n","\n","from model import dope_resnet50, num_joints\n","import postprocess\n","\n","import matplotlib.pyplot as plt"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1WVJ4zv00PH","executionInfo":{"status":"ok","timestamp":1609068061373,"user_tz":-60,"elapsed":742,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["def load_dataset_file(filename):\n","    with gzip.open(filename, \"rb\") as f:\n","        loaded_object = pickle.load(f)\n","        return loaded_object\n","def extract(tmp):\n","  samples = {}\n","  for s in tmp:\n","    seq_id = s[\"name\"]\n","    if seq_id in samples:\n","        assert samples[seq_id][\"name\"] == s[\"name\"]\n","        assert samples[seq_id][\"signer\"] == s[\"signer\"]\n","        assert samples[seq_id][\"gloss\"] == s[\"gloss\"]\n","        assert samples[seq_id][\"text\"] == s[\"text\"]\n","        samples[seq_id][\"sign\"] = torch.cat(\n","            [samples[seq_id][\"sign\"], s[\"sign\"]], axis=1\n","        )\n","    else:\n","        samples[seq_id] = {\n","            \"name\": s[\"name\"],\n","            \"signer\": s[\"signer\"],\n","            \"gloss\": s[\"gloss\"],\n","            \"text\": s[\"text\"],\n","            \"sign\": s[\"sign\"],\n","        }\n","  return samples"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXWWLl1k5n4K","executionInfo":{"status":"ok","timestamp":1609068066411,"user_tz":-60,"elapsed":3234,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["tmp = load_dataset_file('/content/drive/MyDrive/sign_language/data/phoenix14t.pami0.test')\n","samples_test = extract(tmp)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9EvtkHN5wUw","executionInfo":{"status":"ok","timestamp":1609068072525,"user_tz":-60,"elapsed":7062,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["\"\"\" LCR-Net: Localization-Classification-Regression for Human Pose\n","Copyright (C) 2020 NAVER Corp.\n","â€‹\n","This program is free software: you can redistribute it and/or modify\n","it under the terms of the GNU General Public License as published by\n","the Free Software Foundation, either version 3 of the License, or\n","(at your option) any later version.\n","â€‹\n","This program is distributed in the hope that it will be useful,\n","but WITHOUT ANY WARRANTY; without even the implied warranty of\n","MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n","GNU General Public License for more details.\n","â€‹\n","You should have received a copy of the GNU General Public License\n","along with this program.  If not, see <http://www.gnu.org/licenses/>\"\"\"\n","\n","import numpy as np\n","\n","\"\"\"\n","2D boxes are N*4 numpy arrays (float32) with each row at format <xmin> <ymin> <xmax> <ymax>\n","\"\"\"\n","\n","def area2d(b):\n","    \"\"\" compute the areas for a set of 2D boxes\"\"\"\n","    return (b[:,2]-b[:,0]+1)*(b[:,3]-b[:,1]+1)\n","\n","def overlap2d(b1, b2):\n","    \"\"\" compute the overlaps between a set of boxes b1 and 1 box b2 \"\"\"\n","    xmin = np.maximum( b1[:,0], b2[:,0] )\n","    xmax = np.minimum( b1[:,2]+1, b2[:,2]+1)\n","    width = np.maximum(0, xmax-xmin)\n","    ymin = np.maximum( b1[:,1], b2[:,1] )\n","    ymax = np.minimum( b1[:,3]+1, b2[:,3]+1)\n","    height = np.maximum(0, ymax-ymin)   \n","    return width*height          \n","\n","def iou2d(b1, b2):\n","    \"\"\" compute the IoU between a set of boxes b1 and 1 box b2\"\"\"\n","    if b1.ndim == 1: b1 = b1[None,:]\n","    if b2.ndim == 1: b2 = b2[None,:]\n","    assert b2.shape[0]==1\n","    o = overlap2d(b1, b2)\n","    return o / ( area2d(b1) + area2d(b2) - o ) \n","\n","\n","\n","def convert_to_ppi_format(scores, boxes, pose2d, pose3d, score_th=None, tonumpy=True, hand=False):\n","  if score_th is None: \n","        if hand:\n","            score_th = 0.02/(scores.shape[1]-1)\n","        else:\n","            score_th = 0.1/(scores.shape[1]-1)\n","  boxindices, idxlabels = np.where(scores[:,1:]>=score_th) # take the Ndet values with scores over a threshold\n","  s_scores = scores[boxindices, 1+idxlabels] # Ndet (number of estimations with score over score_th)\n","  s_boxes = boxes[boxindices,:]  # Ndetx4 (does not depend on label)\n","  s_pose2d = pose2d[boxindices, idxlabels, : ,:] # NdetxJx2\n","  s_pose3d = pose3d[boxindices, idxlabels, : ,:] # NdetxJx3\n","  return s_scores, s_boxes, s_pose2d, s_pose3d, boxindices, idxlabels \n","\n","indTorsoHead = {13: [4,5,10,11,12], 14: [4,5,10,11,12,13]}\n","indLower     = {13: range(0,4), 14: range(0,4)}  # indices of joints for lower body\n","indUpper     = {13: range(4,13), 14: range(4,14)}  # indices of joints for upper body\n","\n","def LCRNet_PPI_improved(scores, boxes, pose2d, pose3d, resolution, K=None, score_th=None, th_pose3D=0.3, th_iou=0.2, iou_func='bbox', min_mode_score=0.05, th_persondetect=0.002, verbose=False, hand=False):\n","    \"\"\"\n","    this function extends the Pose Proposals Integration (PPI) from LCR-Net in order to also handle hands (J=21), faces (J=84) and bodies with 13 or 14 joints.    \n","    \n","    scores, boxes, pose2d, pose3d are numpy arrays of size [Nboxes x (Nclasses+1)], [Nboxes x 4], [Nboxes x Nclasses x J x 2] and [Nboxes x Nclasses x J x 3] resp.\n","   \n","    resolution: tuple (height,width) containing image resolution\n","    K: number of classes, without considering lower/upper (K=10)\n","    score_th: only apply ppi on pose proposals with score>=score_th (None => 0.1/K)\n","    th_pose3D: threshold for groupind pose in a mode, based on mean dist3D of the joints\n","    th_iou: 2D overlap threshold to group the pose proposals\n","    iou_func: how 2d overlap is defined \n","    min_mode_score: we consider modes with at least this score (save time and do not affect 1st mode)\n","    th_persondetect: we remove final detection whose cumscore is below this threshold\n","\n","    return a list of detection with the following fields:\n","      * score: cumulative score of the detection\n","      * pose2d: Jx2 numpy array\n","      * pose3d: Jx3 numpy array\n","    \"\"\"\n","    if K is None: K = scores.shape[1]\n","    s_scores, s_boxes, s_pose2d, s_pose3d, boxindices, idxlabels = convert_to_ppi_format(scores, boxes, pose2d, pose3d, score_th=score_th, hand=hand)\n","  \n","    H,W = resolution\n","    wh = np.array([[W,H]], dtype=np.float32)\n","    J = s_pose2d.shape[1]\n","    if not J in [13,14]: assert iou_func in ['bbox', 'mje']\n","\n","    # compute bounding boxes from 2D poses truncated by images boundaries\n","    if iou_func=='bbox_torsohead': # using torso+head keypoints only    \n","        xymin = np.minimum( wh-1, np.maximum(0, np.min(s_pose2d[:,indTorsoHead[J],:], axis=1)))\n","        xymax = np.minimum( wh-1, np.maximum(0, np.max(s_pose2d[:,indTorsoHead[J],:], axis=1)))\n","        bbox_headtorso = np.concatenate( (xymin,xymax), axis=1)\n","    else:  # using all keypoints\n","        xymin = np.minimum( wh-1, np.maximum(0, np.min(s_pose2d, axis=1)))\n","        xymax = np.minimum( wh-1, np.maximum(0, np.max(s_pose2d, axis=1)))\n","        bboxes = np.concatenate( (xymin,xymax), axis=1)\n","    # define iou metrics\n","    def compute_overlapping_poses(bboxes, poses2d, a_bbox, a_pose2d, th_iou):\n","        assert a_pose2d.ndim==2 and a_bbox.ndim==1\n","        a_bbox = a_bbox[None,:]\n","        a_pose2d = a_pose2d[None,:,:]\n","        assert bboxes.ndim==2 and poses2d.ndim==3\n","        if iou_func=='bbox' or iou_func=='bbox_torsohead':\n","            iou = iou2d(bboxes, a_bbox)\n","            return np.where( iou>th_iou )[0]\n","        elif iou_func=='torso' or iou_func=='torsoLR':\n","            indices = [4,5,10,11]\n","            lr_indices = [5,4,11,10]\n","        elif iou_func=='torsohead' or iou_func=='torsoheadLR':\n","            indices = [4,5,10,11,12] if J==13 else [4,5,10,11,12,13]\n","            lr_indices = [5,4,11,10,12]  if J==13 else [5,4,11,10,12,13]\n","        elif iou_func=='head':\n","            indices = [12]  if J==13 else [12,13]\n","        elif iou_func=='shoulderhead' or iou_func=='shoulderheadLR':\n","            indices = [10,11,12]  if J==13 else [10,11,12,13]\n","            lr_indices = [11,10,12]  if J==13 else [11,10,12,13]\n","        elif iou_func=='mje':\n","            indices = list(range(J))\n","        else:\n","            raise NotImplementedError('ppi.py: unknown iou_func')\n","        indices = np.array(indices, dtype=np.int32)\n","        if iou_func.endswith('LR'):\n","            lr_indices = np.array(lr_indices, dtype=np.int32)\n","            a = np.minimum( np.mean(np.sqrt( np.sum( (poses2d[:,   indices,:]-a_pose2d[:,indices,:])**2, axis=2)), axis=1),\n","                            np.mean(np.sqrt( np.sum( (poses2d[:,lr_indices,:]-a_pose2d[:,indices,:])**2, axis=2)), axis=1) )\n","        else:\n","            a = np.mean(np.sqrt(np.sum( (poses2d[:,indices,:]-a_pose2d[:,indices,:])**2, axis=2)), axis=1)\n","        b = 2*np.max( a_bbox[:,2:4]-a_bbox[:,0:2]+1 )\n","        return np.where( a/b < th_iou)[0]\n","\n","  \n","    # group pose proposals according to 2D IoU\n","    Persons = [] # list containing the detected people, each person being a tuple ()\n","    remaining_pp = range( s_pose2d.shape[0] )\n","    while len(remaining_pp)>0:\n","        # take highest remaining score\n","        imax = np.argmax( s_scores[remaining_pp] )\n","        # consider the pose proposals with high 2d overlap\n","        this = compute_overlapping_poses( s_boxes[remaining_pp,:], s_pose2d[remaining_pp,:], s_boxes[remaining_pp[imax],:], s_pose2d[remaining_pp[imax],:,:], th_iou)\n","        this_pp = np.array(remaining_pp, dtype=np.int32)[this]\n","        # add the person and delete the corresponding pp\n","        Persons.append( (this_pp, np.sum(s_scores[this_pp])) )\n","        remaining_pp = [p for p in remaining_pp if not p in this_pp]\n","    if verbose: print(\"{:d} persons/groups of poses found\".format(len(Persons)))\n","\n","\n","    Detected = []\n","    mode_isright = []\n","    mode_isleft = []\n","    # find modes for each person\n","    for iperson, (pplist, cumscore) in enumerate(Persons):\n","\n","        remaining_pp = list(pplist.copy()) # create a copy, list of pp that are not assigned to any mode\n","        Modes = []\n","\n","        while len(remaining_pp)>0:\n","\n","            # next anchor pose mode is defined as the top regscore among unassigned poses\n","            imax = np.argmax( s_scores[remaining_pp] )\n","            maxscore = s_scores[remaining_pp[imax]]\n","            if maxscore<min_mode_score and len(Modes)>0: break # stop if score not sufficiently high and already created a mode\n","            # select PP (from the entire set) close to the center of the mode\n","            mode_pose3D = s_pose3d[ remaining_pp[imax], :,:]\n","            #dist3D = np.mean( np.sqrt( (mode_pose3D[ 0:13]-regpose3d[pplist, 0:13])**2 + \\\n","            #                           (mode_pose3D[13:26]-regpose3d[pplist,13:26])**2 + \\\n","            #                           (mode_pose3D[26:39]-regpose3d[pplist,26:39])**2 ), axis=1)\n","            dist3D = np.mean(np.sqrt( np.sum( (mode_pose3D-s_pose3d[pplist,:,:])**2, axis=2)), axis=1)\n","            this = np.where( dist3D < th_pose3D )[0]\n","            \n","            # compute the output for this mode\n","            this_pp = pplist[this]            \n","            weights = s_scores[this_pp]            \n","            \n","            # upper body is average weights by the scores\n","            hand_isright = None\n","            if J in [13,14]:\n","                p3d = np.empty( (J,3), dtype=np.float32)\n","                p2d = np.empty( (J,2), dtype=np.float32)\n","                cumscore = np.sum(weights)\n","                p3d[ indUpper[J], :] = np.sum(weights[:,None,None] * s_pose3d[this_pp,:,:][:,indUpper[J],:], axis=0) / cumscore\n","                p2d[ indUpper[J], :] = np.sum(weights[:,None,None] * s_pose2d[this_pp,:,:][:,indUpper[J],:], axis=0) / cumscore\n","                \n","                assert idxlabels is not None\n","                # for lower body, we downweight upperbody scores\n","                this_ub = np.where(idxlabels[this_pp]> K)[0] # anchor pose for upper body\n","                weights[ this_ub ] *= 0.1\n","                cumscoreBot = np.sum(weights)\n","                p3d[ indLower[J], :] = np.sum(weights[:,None,None] * s_pose3d[this_pp,:,:][:,indLower[J],:], axis=0) / cumscoreBot\n","                p2d[ indLower[J], :] = np.sum(weights[:,None,None] * s_pose2d[this_pp,:,:][:,indLower[J],:], axis=0) / cumscoreBot\n","            else:\n","                cumscore = np.sum(weights)\n","                p3d = np.sum(weights[:,None,None] * s_pose3d[this_pp,:,:], axis=0) / cumscore\n","                p2d = np.sum(weights[:,None,None] * s_pose2d[this_pp,:,:], axis=0) / cumscore\n","                if J==21:\n","                    hand_isright = (idxlabels[imax] < K)\n","              \n","            this_mode = {'score': cumscore, 'pose3d': p3d, 'pose2d': p2d}\n","            if hand_isright is not None: this_mode['hand_isright'] = hand_isright\n","            Modes.append( this_mode ) \n","            \n","            # remove pp from the list to process\n","            remaining_pp = [p for p in remaining_pp if not p in this_pp]\n","        if verbose: print(\"Person {:d}/{:d} has {:d} mode(s)\".format(iperson+1, len(Persons), len(Modes)))\n","        if hand is False: \n","            # keep the main mode for each person, only if score is sufficient high\n","            modes_score = np.array([m['score'] for m in Modes])\n","            bestmode = np.argmax( modes_score )\n","            if modes_score[bestmode] > th_persondetect:\n","                Detected.append( Modes[bestmode] )\n","            else:\n","                if verbose: print(\"\\tdeleting this person because of too low score\")\n","        else:\n","            mode_isright += [m for m in Modes if m['hand_isright']==True]\n","            mode_isleft += [m for m in Modes if m['hand_isright']==False]\n","    if hand is True:\n","      res = mode_isright + mode_isleft\n","      res.sort(key=lambda d: d['score'], reverse=True)\n","      Detected.append(res[0])\n","\n","    if verbose: print('{:d} person(s) detected'.format(len(Detected)))\n","    # sort detection according to score\n","    Detected.sort( key=lambda d: d['score'], reverse=True)\n","    return Detected\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","     \n","# load model\n","ckpt_fname = '/content/drive/MyDrive/sign_language/models/DOPE_v1_0_0.pth.tgz'\n","ckpt = torch.load(ckpt_fname, map_location=device)\n","ckpt['dope_kwargs']['rpn_post_nms_top_n_test'] = 1000\n","model = dope_resnet50(**ckpt['dope_kwargs'])\n","if ckpt['half']: \n","  model = model.half()\n","model = model.eval()\n","model.load_state_dict(ckpt['state_dict'])\n","model = model.to(device)\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"SSr3ur3i535D","executionInfo":{"status":"ok","timestamp":1609068072526,"user_tz":-60,"elapsed":3316,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["shape2d = {'face': (84, 2), 'hand': (21, 2), 'body': (13, 2)}\n","shape3d =  {'face': (84, 3), 'hand': (21, 3), 'body': (13, 3)}\n","def extractor(image):\n","    if isinstance(image, str):     \n","      image = np.array(Image.open(image))[...,:3]\n","    imlist = [ToTensor()(image).to(device)]\n","    if ckpt['half']: imlist = [im.half() for im in imlist]\n","    resolution = imlist[0].size()[-2:]\n","    \n","    # forward pass of the dope network\n","    with torch.no_grad():\n","        outputs = model(imlist, None)\n","        results, add_features = outputs[0][0], outputs[1]\n","    # postprocess results (pose proposals integration, wrists/head assignment)\n","    parts = ['body','hand','face']\n","    res = {k: v.float().data.cpu().numpy() for k,v in results.items()}\n","    detections = {}\n","    for part in parts:\n","      detections[part] = LCRNet_PPI_improved(res[part+'_scores'], res['boxes'], res[part+'_pose2d'], res[part+'_pose3d'], resolution, **ckpt[part+'_ppi_kwargs'], hand=(part=='hand'))\n","    # assignment of hands and head to body\n","    detections, body_with_wrists, body_with_head = postprocess.assign_hands_and_head_to_body(detections)\n","    for part in parts:\n","      detections[part] += [{'score': -10, 'pose3d': np.zeros(shape3d[part]), 'pose2d': np.zeros(shape2d[part])}]\n","    return detections,  add_features"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vMumbu2s56SW","executionInfo":{"status":"ok","timestamp":1609068076039,"user_tz":-60,"elapsed":999,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"a57bae2a-ff8f-478e-e31a-d12e51b7d5fa"},"source":["abs_path = '/content/drive/MyDrive/sign_language/data/'\n","#path_vid = 'train/01August_2011_Monday_heute-4858.mp4'\n","path_vid = 'dev/11August_2010_Wednesday_tagesschau-3.mp4'\n","vidcap = cv2.VideoCapture(abs_path + path_vid)\n","success,image = vidcap.read()\n","count = 0\n","images = []\n","while success:\n","  images.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))     # save frame as JPEG file      \n","  success,image = vidcap.read()\n","  count += 1\n","len(images)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["111"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"5pw3uHs456x5","executionInfo":{"status":"ok","timestamp":1609068151378,"user_tz":-60,"elapsed":801,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["def capture_frames(path_vid):\n","  vidcap = cv2.VideoCapture(path_vid)\n","  success,image = vidcap.read()\n","  count = 0\n","  frames = []\n","  while success:\n","    frames.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))    \n","    success,image = vidcap.read()\n","    count += 1\n","  return frames\n","\n","def extract_features_from_frames(frames):\n","  dct = {'hand_2d': [], 'hand_3d': [], 'body_2d': [], 'body_3d': [], \n","          'face_2d': [], 'face_3d': []} #'preroi': [], 'postroi': []}\n","  for frame in frames:\n","    keypoints, features = extractor(frame)\n","    #dct['preroi'].append(features[1]['0'].detach().cpu().numpy())\n","    #dct['postroi'].append(features[0].detach().cpu().numpy())\n","    del features\n","    torch.cuda.empty_cache()\n","    dct['hand_2d'].append(keypoints['hand'][0]['pose2d'])\n","    dct['hand_3d'].append(keypoints['hand'][0]['pose3d'])\n","    dct['face_2d'].append(keypoints['face'][0]['pose2d'])\n","    dct['face_3d'].append(keypoints['face'][0]['pose3d'])\n","    dct['body_2d'].append(keypoints['body'][0]['pose2d'])\n","    dct['body_3d'].append(keypoints['body'][0]['pose3d'])\n","  return {k: np.array(v) for k,v in dct.items()}"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["94b118cfd1c3457899a4b48f9f729fa9","8d5f35c09c1f4b3b9544e022cc4904b9","c898455c09f849d7948121528c07cdc5","18b42311cc95479880501eb6380a01e7","eef564e7cfac4591a7c0a46cd34930f4","ed03b27b9df44255be3e795a0d67d42d","e5dc51782d794644a0073d835fe02509","cd45c382d919409cabe133c8d0d0720c"]},"id":"SsrRXs9w5-Ci","executionInfo":{"status":"ok","timestamp":1609072049941,"user_tz":-60,"elapsed":3897120,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"7d310ce5-40c4-4f71-bbad-475078997994"},"source":["from tqdm import tqdm_notebook as tqdm\n","already_done = []\n","test_features = {k: {'name': v['name']} for k,v in samples_test.items()}\n","abs_path = '/content/drive/MyDrive/sign_language/data/'\n","for key in tqdm(samples_test.keys()):\n","  path_vid = abs_path + key + '.mp4'\n","  frames = capture_frames(path_vid)\n","  assert len(frames) == len(samples_test[key]['sign'])\n","  test_features[key].update(extract_features_from_frames(frames))\n","  already_done.append(key)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  \"\"\"\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94b118cfd1c3457899a4b48f9f729fa9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=642.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["680fbaead58649308ce5a420b93e8dbf","5e510764c7484e149f8019bc6cec1931","c6db1b979fb641d2bb66078cb1528dc2","12485cc6b4d54323a0ef0133fe25aa1c","0d2dd583a0954ddf84e5ad83bc77f5bc","0c6466ec6fc04c16bcf9ea2fc95f194b","2dd34d54af354230bc5f03125f65b36a","5233e2d1d58749bfb9dc9314b0c9464c"]},"id":"OIWeWyZ8K3Re","executionInfo":{"status":"ok","timestamp":1609072136199,"user_tz":-60,"elapsed":832,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"825f595d-c6d0-487b-d8fc-f85155ba1e87"},"source":["samples_test_copy = samples_test.copy()\n","for key in tqdm(samples_test.keys()):\n","  samples_test_copy[key].update(test_features[key])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  \n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"680fbaead58649308ce5a420b93e8dbf","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=642.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GWdEvVyuLHq4","executionInfo":{"status":"ok","timestamp":1609072151844,"user_tz":-60,"elapsed":886,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["def save(object, filename, protocol = 0):\n","        \"\"\"Saves a compressed object to disk\n","        \"\"\"\n","        file = gzip.GzipFile(filename, 'wb')\n","        file.write(pickle.dumps(object, protocol))\n","        file.close()"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNITCGrNLJFx","executionInfo":{"status":"ok","timestamp":1609072411673,"user_tz":-60,"elapsed":229708,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["save(samples_test_copy, '/content/drive/MyDrive/sign_language/data/phoenix14t.pami0.testadd')"],"execution_count":14,"outputs":[]}]}